{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c42347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "def load_db(file, chain_type, k):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # define embedding\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    \n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    \n",
    "    return qa \n",
    "\n",
    "chat_history = []\n",
    "answer = \"\"\n",
    "db_query = \"\"\n",
    "db_response = []\n",
    "\n",
    "file = st.sidebar.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
    "\n",
    "if file is not None:\n",
    "    qa = load_db(file, \"stuff\", 4)\n",
    "\n",
    "conversation_input = st.text_input('Enter text hereâ€¦')\n",
    "\n",
    "if conversation_input:\n",
    "    result = qa({\"question\": conversation_input, \"chat_history\": chat_history})\n",
    "    chat_history.extend([(conversation_input, result[\"answer\"])])\n",
    "    db_query = result[\"generated_question\"]\n",
    "    db_response = result[\"source_documents\"]\n",
    "    answer = result['answer'] \n",
    "\n",
    "    st.write('User:', conversation_input)\n",
    "    st.write('ChatBot:', answer)\n",
    "\n",
    "if db_query:\n",
    "    st.write(\"Last question to DB:\", db_query)\n",
    "\n",
    "if db_response:\n",
    "    st.write(\"Result of DB lookup:\")\n",
    "    for doc in db_response:\n",
    "        st.write(doc)\n",
    "\n",
    "st.write(\"Current Chat History\")\n",
    "for exchange in chat_history:\n",
    "    st.write(exchange)\n",
    "\n",
    "if st.button(\"Clear History\"):\n",
    "    chat_history.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
